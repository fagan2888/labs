{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression in scikit-learn and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to implement a simple multi-class logistic regression in both scikit-learn and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the Iris data set - a 150 instance, 3-class data set with 4 features.\n",
    "\n",
    "For now, we won't bother with the standard train/test splits - we just want a model that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.data.shape)\n",
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our target $y$ is encoded as a single array with classes as its values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the built-in `LogisticRegression` classifier in scikit-learn. To make this implementation consistent with our PyTorch formulation, we will slightly modify the defaults - namely modifying the multi-class calculation to use a softmax, and turning off the regularization.\n",
    "\n",
    "(If you do not know what 'regularization' means, it will be covered in a later lecture. For now, just know it is an additional part of the model we are not concerned with.)\n",
    "\n",
    "First, we set up the model with our desired arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sk_model = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=1000000,\n",
    "    C=np.finfo(np.float).max,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_model.fit(X=iris.data, y=iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can predict probabilities that each sample is in each class. Note that the probabilities here will be *very* high, because we are clearly overfitting to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_model.predict_proba(X=iris.data)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import some modules from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement a model in PyTorch -- as an `nn.Module`.\n",
    "\n",
    "A `nn.Module` can really be any function, but it is often used to implement layers, functions and models. Note that you can also nest modules.\n",
    "\n",
    "Importantly, modules need to have their `forward()` method overridden, and very often you will want to override the `__init__` method as well. \n",
    "\n",
    "The `__init__` method sets up the module, akin to how we set up the `LogisticRegression` model above with some arguments. This is also often where the internal modules and parameters are initialized.\n",
    "\n",
    "The `forward` method defines what happens when you *apply* the module.\n",
    "\n",
    "In the background, PyTorch makes use of your code in the forward method and determines how to implement back-propagation with it - but all you need to do is to define the forward pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionPyTorch(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        n_in: Number of features\n",
    "        n_out: Number of output classes\n",
    "        \"\"\"\n",
    "        # Initialize the parent class - this is a Python requirement\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up out linear layer. This initializes the weights\n",
    "        # Note that self.linear is itself a nn.Module, nested within\n",
    "        #   this module\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        \n",
    "        # Explicitly initialize the weights with the initialization\n",
    "        #   scheme we want.\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input data [N, k]\n",
    "        ---\n",
    "        Returns: log probabilities of each class [N, c]\n",
    "        \"\"\"\n",
    "        # Apply the linear function to get our logit (real numbers)\n",
    "        logit = self.linear(x)\n",
    "        \n",
    "        # Apply log_softmax to get logs of normalized probabilities\n",
    "        return F.log_softmax(logit, dim=1)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # Use some specific initialization schemes\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.uniform_(self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(iris.data).float()\n",
    "y = torch.from_numpy(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List to record our loss over time\n",
    "loss_val_ls = []\n",
    "\n",
    "# Initialize our model. Note we need to provide n_in and n_out\n",
    "pt_model = LogisticRegressionPyTorch(n_in=x.shape[1], n_out=3)\n",
    "\n",
    "# Set up our criterion - our loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Set up our optimizer. We need to tell the optimizer which\n",
    "#   parameters it will optimize over (which parameters it is\n",
    "#   allowed to modify).\n",
    "optimizer = optim.Adam(pt_model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a quick check, lets investigate the number of parameters in our model:\n",
    "for param in pt_model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run over many iterations!\n",
    "for i in range(10000):\n",
    "    \n",
    "    # Always zero-out the gradients managed by your optimizer\n",
    "    # PyTorch does not automatically zero-out your gradients\n",
    "    #   You can also do pt_model.zero_grad() in this case.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # !! Put model into training mode. This does not do anything\n",
    "    #   in a simple Logistic Regression model, but will be important \n",
    "    #   later. (See: Dropout)\n",
    "    pt_model.train()\n",
    "    \n",
    "    # Compute the predicted log-probabilities\n",
    "    y_hat = pt_model(x)\n",
    "    \n",
    "    # Compute the loss\n",
    "    train_loss = criterion(y_hat, y)\n",
    "    \n",
    "    # Back-propagate the gradients to the parameters\n",
    "    train_loss.backward()\n",
    "    \n",
    "    # Apply the gradient updates to the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Recompute the loss in evaluation mode, and record it.\n",
    "    # Again, this does not do anything here, but will be important later.\n",
    "    # Since we are evaluating, we will also tell PyTorch not to\n",
    "    #   compute gradients.\n",
    "    pt_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_hat = pt_model(x)\n",
    "        eval_loss = criterion(y_hat, y)\n",
    "        \n",
    "    # Record the loss\n",
    "    # Note that 'loss' is a Tensor, but loss.item() is a number\n",
    "    loss_val_ls.append(eval_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_val_ls)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Log Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See our final loss\n",
    "loss_val_ls[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm that our model is doing the right thing, we do a quick test.\n",
    "# We create a new logistic regression model in PyTorch, but we\n",
    "#   insert the weights learned from our scikit-learn model.\n",
    "# We compute the loss similarly.\n",
    "# Note that it is numerically similar to the loss above!\n",
    "# (See if you can understand what every line is doing.)\n",
    "with torch.no_grad():\n",
    "    blank_model = LogisticRegressionPyTorch(n_in=x.shape[1], n_out=3)\n",
    "    blank_model.linear.weight.set_(\n",
    "        torch.from_numpy(sk_model.coef_).float()\n",
    "    )\n",
    "    blank_model.linear.bias.set_(\n",
    "        torch.from_numpy(sk_model.intercept_).float()\n",
    "    )\n",
    "    y_hat = blank_model(x)\n",
    "    print(criterion(y_hat, y).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Questions for yourself\n",
    "\n",
    "1. We asserted that the models are roughly equivalent because they reached the same losses. But is this true? Can we directly compare the parameter values? (Try it!) What if the parameter values are different?\n",
    "\n",
    "2. In scikit-learn, you can use `.predict_proba` to compute the predicted probabilities. How do we do the same for our PyTorch model?\n",
    "\n",
    "3. Although we showed that the loss is numerically the same, and you can confirm for yourself that the predictions $\\hat{y}$ are numerically similar between the scikit-learn and PyTorch implementations, if you inspect the actual weights and biases, you will notice that they are different. Why is this the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Things you should look up\n",
    "\n",
    "1. How to perform training on GPUs\n",
    "    * Hint: both model and training data need to be on GPU\n",
    "2. How to incorporate regularization\n",
    "    * Plus: How to record the regularization loss (i.e. not in optimizer)\n",
    "3. How to save / load models\n",
    "    * Hint: `torch.save`, `torch.load`, and use `model.state_dict()`\n",
    "4. The difference between, e.g. `nn.LogSoftmax()` and `F.log_softmax()`\n",
    "5. The difference between `nn.NLLLoss`, and `nn.CrossEntropyLoss`, and when to use each one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If anything is unclear, please come to office hours!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
