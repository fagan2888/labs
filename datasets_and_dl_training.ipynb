{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and Deep Learning Training Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implement a simple Logistic Regression model using a standard deep learning training workflow in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a digits data set. Each $x$ is an 8x8 matrix representing a hand-written digits, and the $y$ is which of the 10 digits it represented.\n",
    "\n",
    "**Note**: This is *not* MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = datasets.load_digits()\n",
    "print(raw_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_data.data.shape)\n",
    "print(raw_data.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(raw_data.data[0].reshape(8, 8), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to split our data into train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(raw_data.data, raw_data.target, test_size=0.2)\n",
    "x_train, x_val, y_train, y_val = \\\n",
    "    train_test_split(x_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_val.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed - two things about deep learning training workflows.\n",
    "\n",
    "Unlike in the case of regressions (as before), we often cannot fit all the data into memory--particularly when training on GPUs, which often have less memory. Hence, we often train the models iteratively in **batches** (see: *minibatch gradient descent*).\n",
    "\n",
    "Because we do gradient descent, we often also go over the data multiple times--in multiple **epochs**. We need to specify how many epochs to train for (later, you will learn other ways to step epochs early, or potentially not use epochs at all).\n",
    "\n",
    "Here, we can easily fit all the data into memory, but we will pretend we cannot, and set our batch-size per gradient descent step to 32--so we're training on 32 instances per step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are doing to subclass PyTorch's `Dataset` class. A Dataset class can be used to represent any kind of data. Importantly, you need to implement `__getitem__` and `__len__` methods. \n",
    "\n",
    "`__getitem__` in particular has a fixed signature, where given a numerical index, it returns the corresponding data for that instance. \n",
    "\n",
    "That is all you need to do to define the Dataset. PyTorch handles the rest in terms of converting to Tensors and batching - in `DataLoader`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        assert len(x) == len(y)\n",
    "        self.length = len(x)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a data set for our train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(x_train, y_train)\n",
    "val_dataset = MyDataset(x_val, y_val)\n",
    "test_dataset = MyDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a DataLoader for each data set. Note that we often want to shuffle our training data when we iterate over it, but not necessarily the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy our model from the `logistic_regression` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionPyTorch(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        n_in: Number of features\n",
    "        n_out: Number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input data [N, k]\n",
    "        ---\n",
    "        Returns: log probabilities of each class [N, c]\n",
    "        \"\"\"\n",
    "        logit = self.linear(x)\n",
    "        return F.log_softmax(logit, dim=1)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.uniform_(self.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create 3 functions here:\n",
    "\n",
    "1. A training method\n",
    "2. An evaluation method\n",
    "3. A method for computing accuracy\n",
    "\n",
    "In both `do_train` and `do_eval`, we iterate over our provided DataLoader, and carry out the forward pass. Note that `x` and `y` are already neatly batched into the correct batch size and converted to Tensors.\n",
    "\n",
    "Note that `do_train` and `do_eval` do have some overlap--but are also quite different. (See if you can spot all the differences.) Most importantly, we need to perform backpropagation in `do_train`, and in `do_eval` we want to record the outputs. It is possible to combine the two, but the function can get quite ugly--this is up to your personal taste.\n",
    "\n",
    "Exercise: Note that we are carrying around the criterion/model around. Maybe we could turn this into a giant class instead? :)\n",
    "\n",
    "**Make sure you understand *every line* of these methods.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x.float())\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accurately compute loss, because of different batch size\n",
    "        loss_val += loss.item() * len(x) / len(dataloader.dataset)\n",
    "    return loss_val\n",
    "\n",
    "def do_eval(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    loss_val = 0\n",
    "    y_ls = []\n",
    "    y_hat_ls = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            y_hat = model(x.float())\n",
    "            loss = criterion(y_hat, y)\n",
    "            # Accurately compute loss, because of different batch size\n",
    "            loss_val += loss.item() * len(x) / len(dataloader.dataset)\n",
    "            y_hat_ls.append(y_hat)\n",
    "            y_ls.append(y)\n",
    "    optimizer.zero_grad()\n",
    "    return loss_val, torch.cat(y_hat_ls, dim=0), torch.cat(y_ls, dim=0)\n",
    "\n",
    "def acc(model, dataloader, criterion):\n",
    "    _, pred, true = do_eval(\n",
    "        model=model, \n",
    "        dataloader=dataloader,\n",
    "        criterion=criterion,\n",
    "    )\n",
    "    return (torch.exp(pred).max(1)[1] == true).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our model, criterion and optimizer. We also want to record our training and validation losses over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "model = LogisticRegressionPyTorch(\n",
    "    n_in=raw_data.data.shape[1], \n",
    "    n_out=len(raw_data.target_names),\n",
    ")\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the core of our training - we train, and also compute validation loss at each epoch.\n",
    "\n",
    "Note: In some code bases you will often see the core training loop have all sorts of logic here (e.g. batching, data conversion, loss computation, logging, etc). I recommend you refactor those to separate functions/methods, and keep your core loop as clean as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pro-tip: Make your core loop CLEAN\n",
    "for epoch in tqdm.trange(N_EPOCHS):\n",
    "    train_loss = do_train(\n",
    "        model=model, \n",
    "        criterion=criterion,\n",
    "        dataloader=train_dataloader,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    val_loss, val_pred, val_true = do_eval(\n",
    "        model=model, \n",
    "        criterion=criterion,\n",
    "        dataloader=val_dataloader,\n",
    "    )\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot our training and validation loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history_df = pd.DataFrame({\n",
    "    \"train\": train_loss_history,\n",
    "    \"val\": val_loss_history,\n",
    "})\n",
    "loss_history_df.plot(alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute our training, validation and test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(model, train_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(model, val_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc(model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Questions\n",
    "\n",
    "1. Is accuracy the best metric to evaluate our models?\n",
    "2. How many lines of code do you need to add to convert the logistic regression model into a deep neural network? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Things you should look up\n",
    "\n",
    "1. How to perform training on GPUs\n",
    "    * Hint: both model and training data need to be on GPU\n",
    "2. How to incorporate regularization\n",
    "    * Plus: How to record the regularization loss (i.e. not in optimizer)\n",
    "3. How to save / load models\n",
    "    * Hint: `torch.save`, `torch.load`, and use `model.state_dict()`\n",
    "4. The difference between, e.g. `nn.LogSoftmax()` and `F.log_softmax()`\n",
    "5. The difference between `nn.NLLLoss`, and `nn.CrossEntropyLoss`, and when to use each one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If anything is unclear, please come to office hours!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
